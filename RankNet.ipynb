{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import collections\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import copy\n",
    "import itertools\n",
    "import pdb\n",
    "from sklearn.externals.joblib import Memory\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2000 queries per each data subset. Use less for intitial training\n",
    "max_que =2000\t# DOWNSCALE: only take this many queries per fold\n",
    "max_fea = 136\t# DOWNSCALE: only use this many features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mem = Memory(\"./mycache\")\t# cache to binary for faster load\n",
    "@mem.cache\n",
    "def get_data(file):\n",
    "    return load_svmlight_file(file,query_id=True)\n",
    "\n",
    "def extract_data(phase,folds):\t# put into useful structures\n",
    "    fea = np.empty((0,max_fea))\n",
    "    sco = []\n",
    "    feat_by_query = []\n",
    "    q_id = []\n",
    "    que = np.empty((0,),dtype=np.int32)\n",
    "    M = 0\n",
    "    relevance=[]   \n",
    "    for i in folds:\n",
    "        features,scores,queries = S[i]\n",
    "        relevance.extend(scores)\n",
    "        scores = [np.array(x,dtype=np.int32) for x in separate_by_query(scores,queries)[:max_que]]\n",
    "        stop = sum(len(query) for query in scores)\t# number of rows for max_que\n",
    "        q_id.append(stop)        \n",
    "        M += stop\n",
    "        fea = np.concatenate((fea,features[:stop,:max_fea].toarray()))\n",
    "        sco.extend(scores)\n",
    "        \n",
    "        \n",
    "        que = np.concatenate((que,queries[:stop]))\n",
    "    return fea,sco,que,M,len(sco),q_id, np.asarray(relevance)\n",
    "\n",
    "def separate_by_query(scores,queries):\t# from 2 lists to 1 list of lists\n",
    "    scoreslist = []\n",
    "    total_scores = []\n",
    "    current_query = None\n",
    "    for score, query in zip(scores, queries):\n",
    "        if query != current_query:\t# works since query info is contiguous\n",
    "            scoreslist.append([])\n",
    "            current_query = query\n",
    "        scoreslist[-1].append(score)\n",
    "    return [np.array(x,dtype=np.float32) for x in scoreslist]\n",
    "\n",
    "def feat_separate_by_query(scores,queries):\t# from 2 lists to 1 list of lists\n",
    "    scoreslist = []\n",
    "    total_scores = []\n",
    "    current_query = None\n",
    "    for score, query in zip(scores, queries):\n",
    "        if query != current_query:\t# works since query info is contiguous\n",
    "            scoreslist.append([])\n",
    "            current_query = query\n",
    "        scoreslist[-1].append(score)\n",
    "    return [np.array(x,dtype=np.float32) for x in scoreslist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n"
     ]
    }
   ],
   "source": [
    "S = [None]\t# dummy so index isn't confusing\n",
    "for i in range(5):\n",
    "    S.append(get_data('./MSLR-WEB10K/S'+str(i+1)+'.txt'))\n",
    "train_fea,train_sco,train_que,M_train,Q_train,Q_id_train,relevance_train = extract_data('train',[1,2,3,4])\n",
    "#vali_fea, vali_sco, vali_que, M_vali, Q_vali,Q_id_vali, relevance_valid= extract_data('vali',[3])\n",
    "test_fea, test_sco, test_que, M_test, Q_test, Q_id_test,relevance_test  = extract_data('test',[5])\n",
    "print('data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# need placeholders for the inputs to train, x, and the true labels\n",
    "max_fea = 136\n",
    "x  = tf.placeholder( tf.float32, shape =[None, max_fea],name ='x_labels')\n",
    "# Create a placeholder for the y labels\n",
    "relevance_scores= tf.placeholder( tf.float32, shape = [None, 1],name = 'y_labels')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Need to save the model, weights and biases varibles\n",
    "name = 'test'\n",
    "# Suggested Directory to use\n",
    "save_MDir = 'models/'\n",
    "\n",
    "\n",
    "#create the directory if it does not exist already\n",
    "if not os.path.exists(save_MDir):\n",
    "    os.makedirs(save_MDir)\n",
    "\n",
    "save_model = os.path.join(save_MDir,'best_accuracy_'+name)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def optimize_cost(x, relevance_labels, learning_rate, n_hidden, n_layers):\n",
    "\n",
    "    n_data = tf.shape(x)[0]\n",
    "\n",
    "    def get_variables():\n",
    "        variables = [tf.Variable(tf.random_normal([max_fea, n_hidden], stddev=math.sqrt(2 / (max_fea)))),\n",
    "            tf.Variable(tf.zeros([n_hidden]))]\n",
    "        \n",
    "        if n_layers > 1:\n",
    "            for i in range(n_layers-1):\n",
    "                variables.append(tf.Variable(tf.random_normal([n_hidden, n_hidden], stddev=math.sqrt(2 / (n_hidden)))))\n",
    "                variables.append(tf.Variable(tf.zeros([n_hidden])))\n",
    "                \n",
    "        variables.append(tf.Variable(tf.random_normal([n_hidden, 1], stddev=math.sqrt(2 / (n_hidden)))))\n",
    "        variables.append(tf.Variable(0, dtype=tf.float32))\n",
    "        return variables\n",
    "\n",
    "    def score(x, *vars):\n",
    "        z = tf.contrib.layers.batch_norm(tf.matmul(x, vars[0]) + vars[1])\n",
    "        if n_layers > 1:\n",
    "            for i in range(0,n_layers-1):\n",
    "                z = tf.contrib.layers.batch_norm(tf.matmul(tf.nn.relu(z), vars[2*(i+1)]) + vars[2*(i+1)+1])\n",
    "        return tf.matmul(tf.nn.relu(z), vars[-2]) + vars[-1]\n",
    "\n",
    "    vars = get_variables()\n",
    "    o_ij = score(x, *vars)\n",
    "    S_ij = tf.maximum(tf.minimum(1., relevance_labels - tf.transpose(relevance_labels)), -1.)\n",
    "    \n",
    "    targets = (1 / 2) * (1 + S_ij)\n",
    "    \n",
    "    pairwise_o_ij = o_ij - tf.transpose(o_ij)\n",
    "    cost = tf.nn.sigmoid_cross_entropy_with_logits(pairwise_o_ij, targets)\n",
    "    cost = tf.reduce_mean((tf.ones([n_data, n_data]) - tf.diag(tf.ones([n_data]))) * cost)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    def get_score(sess, feed_dict):\n",
    "        return sess.run(o_ij, feed_dict=feed_dict)\n",
    "\n",
    "    def run_optimizer(sess, feed_dict):\n",
    "        sess.run(optimizer, feed_dict=feed_dict)\n",
    "\n",
    "    return cost, run_optimizer, get_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_NDCG(ranks,scores,level = 10):\n",
    "    \"Get NDCG @10. Takes in outputs from all docs after being passed through net and takes top 10,\"\n",
    "    \" gets there relevance scores and calcuates the NDCG\"\n",
    "    # Takes in outputs from\n",
    "    top_scores = []\n",
    "    top_true_scores = []\n",
    "    # Get top ten ouptputs indexes\n",
    "    for i in range(len(ranks)):\n",
    "        top_scores.append(list(reversed(np.argsort(ranks[i])[-10:].tolist())))\n",
    "        top_true_scores.append(list(reversed(np.argsort(scores[i])[-10:].tolist())))\n",
    "    top_rels = []\n",
    "    top_true_rels = []\n",
    "    \n",
    "    # get the relevance scores of the top 10 predicted and top 10 actual for the ideal dcg\n",
    "    for i in range(len(scores)):\n",
    "        \n",
    "        top_rels.append(list(scores[i][top_scores[i]])) \n",
    "        top_true_rels.append(list(scores[i][top_true_scores[i]])) \n",
    "    ndcg_q = []\n",
    "    \n",
    "    # For each query loop over the top 10 documents and calculate the dcg and ideal dcg\n",
    "    for i in range(len(top_rels)):\n",
    "        current_dcg = 0\n",
    "        current_ideal = 0\n",
    "        \n",
    "        # There exists cases with less than 10 dcouments are associated with the query\n",
    "        if len(top_rels[i])<10: \n",
    "            level = len(top_rels[i])\n",
    "        for j in range(level):\n",
    "            current_dcg += ((2**top_rels[i][j]) - 1)/np.log(j+1+1)\n",
    "            current_ideal+= ((2**top_true_rels[i][j]) - 1)/np.log(j+1+1)\n",
    "            \n",
    "        # calculate the ndcg\n",
    "        if  current_ideal != 0:\n",
    "            ndcg_q.append(current_dcg/current_ideal)\n",
    "        else: ndcg_q.append(0)\n",
    "        level = 10\n",
    "    \n",
    "    return ndcg_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ERR(ranks,scores,level = 10):\n",
    "    \"Get the Expected reciprical Rank\"\n",
    "    top_scores = []\n",
    "    \n",
    "    # Get the top 10 ranked score indexes\n",
    "    for i in range(len(ranks)):\n",
    "        top_scores.append(list(reversed(np.argsort(ranks[i])[-10:].tolist())))\n",
    "        \n",
    "    top_rels = []\n",
    "    # get the relvance scores\n",
    "    for i in range(len(scores)):\n",
    "        top_rels.append(list(scores[i][top_scores[i]]))\n",
    "        \n",
    "    ERR_q = []\n",
    "    for i in range(len(top_rels)):\n",
    "        current_err = 0\n",
    "        if len(top_rels[i])<10: \n",
    "            level = len(top_rels[i])\n",
    " \n",
    "        prod = 0\n",
    "        count = 1\n",
    "        for r in range(level):\n",
    "            R = 2**(top_rels[i][r])-1\n",
    "            prod = (1/count) *(R/2**4)\n",
    "            for j in range (r):\n",
    "                Rj = (2**(top_rels[i][j])-1)/16\n",
    "                prod *= 1-Rj\n",
    "            current_err +=prod\n",
    "            count+=1\n",
    "        ERR_q.append(current_err)\n",
    "        level = 10    \n",
    "    return ERR_q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_random_NDCG(ranks,scores,level = 10):\n",
    "    random_scores = []\n",
    "    top_true_scores = []\n",
    "    for i in range(len(ranks)):\n",
    "        top_true_scores.append(list(reversed(np.argsort(scores[i])[-10:].tolist())))\n",
    "    top_rels = []\n",
    "    top_true_rels = []\n",
    "    #print(len(top_scores))\n",
    "    for i in range(len(scores)):\n",
    "        if len(ranks[i])<10:\n",
    "                top_rels.append(list(ranks[i][:len(ranks[i])]))\n",
    "        else:\n",
    "            top_rels.append(list(ranks[i][:10]))\n",
    "        top_true_rels.append(list(scores[i][top_true_scores[i]])) \n",
    "    ndcg_q = []\n",
    "    #pdb.set_trace()\n",
    "    for i in range(len(top_rels)):\n",
    "        current_dcg = 0\n",
    "        current_ideal = 0\n",
    "        #print('Running query:{}'.format(i+1))\n",
    "        if len(top_rels[i])<10: \n",
    "            level = len(top_rels[i])\n",
    "        #print(level)   \n",
    "        for j in range(level):\n",
    "            current_dcg += ((2**top_rels[i][j]) - 1)/np.log(j+1+1)\n",
    "            current_ideal+= ((2**top_true_rels[i][j]) - 1)/np.log(j+1+1)\n",
    "            #print(top_rels[i][j])\n",
    "            #print(current_dcg)\n",
    "        if  current_ideal != 0:\n",
    "            ndcg_q.append(current_dcg/current_ideal)\n",
    "        else: ndcg_q.append(0)\n",
    "        level = 10\n",
    "    \n",
    "    return ndcg_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_test(this_sco,this_feature,i):\n",
    "    batch_size = 64\n",
    "    test_ranks = []\n",
    "    ind = 0\n",
    "    count_docs = 0\n",
    "    docs_per_query = len(this_sco[i])\n",
    "\n",
    "    n_batches = math.floor(docs_per_query/batch_size)\n",
    "    for batch in range(n_batches):\n",
    "        #print(j)\n",
    "        docs = this_feature[ind:ind+batch_size]\n",
    "        rnks = relevance_train[ind:ind+batch_size]\n",
    "        # index for all scores\n",
    "        ind += batch_size\n",
    "        # index for number of docs passed\n",
    "        count_docs += batch_size\n",
    "        feed_dict = {x: np.array(docs, ndmin=2),\n",
    "                    relevance_scores: np.array(rnks, ndmin=2).T,\n",
    "                       }\n",
    "        s = score(sess, feed_dict)\n",
    "\n",
    "        # get the outputs without optimzing\n",
    "        #out = sess.run(output, feed_dict)\n",
    "        s =list(itertools.chain.from_iterable(s))\n",
    "        #pdb.set_trace()\n",
    "        test_ranks.append(s)\n",
    "\n",
    "\n",
    "\n",
    "        # If the remaining batch is less than the normal batchsize\n",
    "        if batch+1 == n_batches:\n",
    "            if docs_per_query > count_docs:\n",
    "                batch_size = (docs_per_query - ind)\n",
    "                docs = this_feature[ind:ind+batch_size]\n",
    "                rnks = relevance_train[ind:ind+batch_size]\n",
    "                ind += batch_size\n",
    "                feed_dict = {x: np.array(docs, ndmin=2),\n",
    "                    relevance_scores: np.array(rnks, ndmin=2).T,\n",
    "                       }   \n",
    "\n",
    "                s = score(sess, feed_dict)\n",
    "\n",
    "                # get the outputs without optimzing\n",
    "                #out = sess.run(output, feed_dict)\n",
    "                s =list(itertools.chain.from_iterable(s))\n",
    "                test_ranks.append(s)\n",
    "\n",
    "\n",
    "    # case where batch size is too big    \n",
    "    if n_batches == 0:\n",
    "        batch_size = docs_per_query\n",
    "        docs = this_feature[ind:ind+batch_size]\n",
    "        rnks = relevance_train[ind:ind+batch_size]\n",
    "        ind += batch_size\n",
    "        feed_dict = {x: np.array(docs, ndmin=2),\n",
    "            relevance_scores: np.array(rnks, ndmin=2).T,\n",
    "               }   \n",
    "\n",
    "        s = score(sess, feed_dict)\n",
    "\n",
    "        s =list(itertools.chain.from_iterable(s))\n",
    "        test_ranks.append(s)\n",
    "    return list(itertools.chain.from_iterable(test_ranks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_metrics(total_ranks,sco):\n",
    "    ndcg_list = get_NDCG(total_ranks,sco)\n",
    "    ndcg = np.mean(ndcg_list)\n",
    "    err_list = get_ERR(total_ranks,sco)\n",
    "    err = np.mean(err_list)\n",
    "    return ndcg,err,ndcg_list,err_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_features = [train_fea,test_fea]\n",
    "all_sco = [train_sco, test_sco] \n",
    "total_ranks= []\n",
    "epoch=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DO Not run, training will begin otherwise. Continue to load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# taking pairs from the same query\n",
    "learning_rate = 0.001\n",
    "n_hidden = 20\n",
    "n_layers = 3\n",
    "\n",
    "cost, optimizer, score = optimize_cost(x, relevance_scores, learning_rate, n_hidden, n_layers)\n",
    "start = time.time()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "NDCG_epochs,ERR_epochs =[],[]\n",
    "all_features = [train_fea,test_fea]\n",
    "all_sco = [train_sco, test_sco] \n",
    "n_epochs = 50\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        total_ranks= []\n",
    "        query_c = 0\n",
    "        batch_size = 64\n",
    "        this_feature = all_features[0]\n",
    "        this_sco = all_sco[0]\n",
    "        for i in range(len(this_sco)):\n",
    "            indices = np.random.randint(query_c, query_c + len(this_sco[i]), batch_size)\n",
    "            query_c+= len(this_sco[i])\n",
    "            if len(indices) > batch_size:\n",
    "                indices = indices[:batch_size]\n",
    "\n",
    "            optimizer(sess, {\n",
    "                            x: np.array(train_fea[indices], ndmin=2),\n",
    "                            relevance_scores: np.array(relevance_train[indices], ndmin=2).T,})\n",
    "            if i%1000 ==0:\n",
    "                print('Running query {}, total time elapsed: {}'.format(i+1, time.time() - start))\n",
    "                c = sess.run(cost,{x: np.array(train_fea[indices], ndmin=2),\n",
    "                                    relevance_scores: np.array(relevance_train[indices], ndmin=2).T,})\n",
    "                #print(c)\n",
    "        this_feature = all_features[1]\n",
    "        this_sco = all_sco[1]   \n",
    "        ranks = []\n",
    "        batch_size = 64\n",
    "        ind = 0\n",
    "        for i in range(len(this_sco)):\n",
    "            \n",
    "            ranks =get_test(this_sco,this_feature,i)\n",
    "\n",
    "            #append to list for ranks of query\n",
    "            total_ranks.append(ranks)\n",
    "            # Each element is a list of ranks for each phase   \n",
    "        print('caluclating NDCG for epoch: {}'.format(epoch+1))\n",
    "        ndcg , err,ndcg_RN,err_RN = calc_metrics(total_ranks,all_sco[1])\n",
    "        print('NDCG:= {}, ERR:= {}\\n'.format(ndcg,err))\n",
    "        #print('ERR vals at this epoch = {}\\n'.format(err))\n",
    "        NDCG_epochs.append(ndcg)\n",
    "        ERR_epochs.append(err) \n",
    "        print('saving_model..')\n",
    "        saver.save(sess= sess, save_path = save_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# baseline\n",
    "list_nds = []\n",
    "for i in range(50):\n",
    "    random_rank = copy.deepcopy(all_sco[1])\n",
    "    for i in range(len(random_rank)):\n",
    "        r = random.random()\n",
    "        random.shuffle(random_rank[i], lambda:r)\n",
    "    #nd, top_rels = get_random_NDCG(random_rank,all_sco[ndcg])\n",
    "    nd_list = (get_random_NDCG(random_rank,all_sco[1]))\n",
    "    nd = np.mean(nd_list)\n",
    "    list_nds.append(nd_list)\n",
    "    ERR = np.mean(np.nan_to_num(get_ERR(random_rank,all_sco[1])))\n",
    "    #ndcg_list.append(nd)\n",
    "    #err_list.append(ERR)\n",
    "print('NDCG vals fro random top 10 = {}'.format(nd))\n",
    "#print('ERR vals at this epoch = {}, {}, {}\\n'.format(err_list[0],err_list[1],err_list[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# filename1 = './results/RN_ndcg_'+name+'.npy'\n",
    "# filename2 = './results/RN_err_'+name+'_.npy'\n",
    "# filename3 = './results/RN_ndcg_list_'+name+'.npy'\n",
    "# filename4 = './results/RN_err_list_'+name+'.npy'\n",
    "\n",
    "# np.save(filename3,(ndcg_RN) )\n",
    "# np.save(filename4,(err_RN) )\n",
    "# #\n",
    "# #np.save(filename3,np.asarray(get_NDCG(all_ranks[ndcg],all_sco[ndcg])) )\n",
    "# #np.save(filename4,np.asarray(np.nan_to_num(get_ERR(all_ranks[ndcg],all_sco[ndcg]))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the metric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "iterations = n_epochs*len(all_sco[0])\n",
    "t = np.linspace(0,n_epochs-1,n_epochs)\n",
    "fig = plt.figure()\n",
    "\n",
    "plt.plot(t,NDCG_epochs,'b')# plotting t,a separately \n",
    "\n",
    "fig.suptitle('Plot of NDCG@10 on test set over 50', fontsize=20)\n",
    "legend = plt.legend(loc='lower right', shadow=True)\n",
    "plt.xlabel('Epoch', fontsize=18)\n",
    "plt.ylabel('NDCG Value', fontsize=16)\n",
    "plt.show()\n",
    "#fig.savefig('./results/ndcg_50.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "iterations = n_epochs*len(all_sco[0])\n",
    "t = np.linspace(0,n_epochs-1,n_epochs)\n",
    "fig = plt.figure()\n",
    "\n",
    "plt.plot(t,ERR_epochs,'b')# plotting t,a separately \n",
    "\n",
    "fig.suptitle('Plot of ERR@10 on test set over 50', fontsize=20)\n",
    "legend = plt.legend(loc='lower right', shadow=True)\n",
    "plt.xlabel('Epoch', fontsize=18)\n",
    "plt.ylabel('ERR Value', fontsize=16)\n",
    "plt.show()\n",
    "#fig.savefig('./results/err_50.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for significane\n",
    "Use a t-test and wilcoxons to test if there is a significant difference between the results.\n",
    "Calculate a random mean NDCG value for every epoch and take a t test between the random and the actual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t,p =stats.ttest_rel(nd_rand,ndcg_RN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_w,p_w = stats.wilcoxon(nd_list, ndcg_RN, zero_method='wilcox', correction=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Need to save the model, weights and biases varibles\n",
    "name = 'test'\n",
    "# Suggested Directory to use\n",
    "save_MDir = 'models/'\n",
    "save_model = os.path.join(save_MDir,'best_accuracy_'+name)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caluclating NDCG for epoch: 1\n",
      "NDCG:= 0.19210439561591056, ERR:= 0.13830116302775045\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "learning_rate = 0.001\n",
    "n_hidden = 20\n",
    "n_layers = 3\n",
    "init = tf.global_variables_initializer()\n",
    "# need placeholders for the inputs to train, x, and the true labels\n",
    "max_fea = 136\n",
    "x  = tf.placeholder( tf.float32, shape =[None, max_fea],name ='x_labels')\n",
    "# Create a placeholder for the y labels\n",
    "relevance_scores= tf.placeholder( tf.float32, shape = [None, 1],name = 'y_labels')\n",
    "\n",
    "cost, optimizer, score = optimize_cost(x, relevance_scores, learning_rate, n_hidden, n_layers)\n",
    "saver2restore = tf.train.Saver()\n",
    "\n",
    "#saver = tf.train.Saver()\n",
    "\n",
    "start = time.time()\n",
    "NDCG_epochs,ERR_epochs =[],[]\n",
    "all_features = [train_fea,test_fea]\n",
    "all_sco = [train_sco, test_sco] \n",
    "n_epochs = 50\n",
    "epoch = 0\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    saver2restore.restore(sess = sess, save_path= save_model)\n",
    "    this_feature = all_features[1]\n",
    "    this_sco = all_sco[1]   \n",
    "    total_ranks = []\n",
    "    batch_size = 64\n",
    "    ind = 0\n",
    "    for i in range(len(this_sco)):\n",
    "        ranks = []\n",
    "        ranks =get_test(this_sco,this_feature,i)\n",
    "\n",
    "        #append to list for ranks of query\n",
    "        total_ranks.append(ranks)\n",
    "        # Each element is a list of ranks for each phase   \n",
    "    #pdb.set_trace()  \n",
    "    print('caluclating NDCG for epoch: {}'.format(epoch+1))\n",
    "    ndcg , err,ndcg_list,err_list = calc_metrics(total_ranks,all_sco[1])\n",
    "    print('NDCG:= {}, ERR:= {}\\n'.format(ndcg,err))\n",
    "    #print('ERR vals at this epoch = {}\\n'.format(err))\n",
    "    NDCG_epochs.append(ndcg)\n",
    "    ERR_epochs.append(err) \n",
    "    epoch+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:IR]",
   "language": "python",
   "name": "conda-env-IR-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
